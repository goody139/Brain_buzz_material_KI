# -*- coding: utf-8 -*-
"""Brain_buzz_demo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pZ1M23MfSrxTtx-7gqHWh24s1di97gJL

# **Simplified Stable Diffusion**


Stable Diffusion is a text-to-image latent diffusion model created by the researchers and engineers from [CompVis](https://github.com/CompVis), [Stability AI](https://stability.ai/) and [LAION](https://laion.ai/).
It's trained on 512x512 images from a subset of the [LAION-5B](https://laion.ai/blog/laion-5b/) database.
This model uses a frozen CLIP ViT-L/14 text encoder to condition the model on text prompts. With its 860M UNet and 123M text encoder, the model is relatively lightweight and can run on many consumer GPUs.
For the further information about Stable Diffusion and this notebook, check out [Stable Diffusion Website](https://stability.ai/blog/stable-diffusion-public-release).

### Setup

Make sure to use a GPU runtime to run this notebook, so inference is much faster.
If the following command fails, use the `Runtime` menu above and select `Change runtime type`.
"""

!nvidia-smi

"""Then, install `diffusers`,`transformers`,`scipy`, and `ftfy`. `accelerate` is used for faster loading."""

!pip install diffusers==0.11.1
!pip install transformers scipy ftfy accelerate
!pip install anvil-uplink

"""### Stable Diffusion Pipeline

`StableDiffusionPipeline` is an end-to-end inference pipeline that you can use to generate images from text with just a few lines of code.

First, load the pre-trained weights of all components of the model. Stable Diffusion version 1.4 ([CompVis/stable-diffusion-v1-4](https://huggingface.co/CompVis/stable-diffusion-v1-4)) is used for this notebook. In addition to the modelid [CompVis/stable-diffusion-v1-4](https://huggingface.co/CompVis/stable-diffusion-v1-4), we're also passing a specific `revision` and `torch_dtype` to the `from_pretrained` method.

To make sure that it works for every free Google Colab, the weights from the half-precision branch [`fp16`](https://huggingface.co/CompVis/stable-diffusion-v1-4/tree/fp16) are loaded and also tell `diffusers` to expect the weights in float16 precision by passing `torch_dtype=torch.float16`.

"""

# network import
import torch
from diffusers import StableDiffusionPipeline

# anvil interface imports
import anvil.server
anvil.server.connect("server_H25NBV2WLDYQHXHJGONGUORW-KCKQ4L2USRALWBPW")

pipe = StableDiffusionPipeline.from_pretrained("CompVis/stable-diffusion-v1-4", torch_dtype=torch.float16)

"""Then, move the pipeline to GPU to have faster inference."""

pipe = pipe.to("cuda")

import io

def img_to_media_obj(img):
  img_byte_arr = io.BytesIO()
  img.save(img_byte_arr, format='JPEG')
  img_byte_arr = img_byte_arr.getvalue()
  media_obj = anvil.BlobMedia(content_type="image/jpeg", content=img_byte_arr)
  return media_obj

"""And we are ready to generate images:"""

@anvil.server.callable
def generate_image(user_input):
  # Write a prompt message about the image that you want to generate(This gives slightly different images with the same prompt)
  image = pipe(user_input).images[0]  # image here is in [PIL format](https://pillow.readthedocs.io/en/stable/)

  # Now to display an image, you can either save it such as:
  #image.save(f"a flying platypus.png")

  # or if you're in a google colab you can directly display it with
  #return image
  return img_to_media_obj(image)

anvil.server.wait_forever()